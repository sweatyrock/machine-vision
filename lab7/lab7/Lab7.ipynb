{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pattern recognition: Lab 7\n",
    "### Tasks:\n",
    "* Plot the error\n",
    "* Model XOR with the help of sigmoid\n",
    "* Add moments rule to learning equation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "k = 1\n",
    "def sigmoid(x):\n",
    "    return 1.0/(1.0 + np.exp(-k*x))\n",
    "\n",
    "def sigmoid_prime(x):\n",
    "    return x*(1.0-x)\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def tanh_prime(x):\n",
    "    return 1.0 - x**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs: 0\n",
      "epochs: 10000\n",
      "epochs: 20000\n",
      "epochs: 30000\n",
      "epochs: 40000\n",
      "epochs: 50000\n",
      "epochs: 60000\n",
      "epochs: 70000\n",
      "epochs: 80000\n",
      "epochs: 90000\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAEKCAYAAAAW8vJGAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7900c544a8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0] [ 0.01459753  0.0144866   0.00134394]\n",
      "[0 1] [ 0.98314203  0.98210591  0.99085683]\n",
      "[1 0] [ 0.98316873  0.9821339   0.99087281]\n",
      "[1 1] [ 0.01848597  0.01848942  0.01058104]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as pltimg\n",
    "\n",
    "class NeuralNetwork:\n",
    "\n",
    "    def __init__(self, layers):\n",
    "        self.activation = sigmoid\n",
    "        self.activation_prime = sigmoid_prime\n",
    "\n",
    "# Set weights\n",
    "        self.weights = []\n",
    "# layers = [2,2,1]\n",
    "# range of weight values (-1,1)\n",
    "# input and hidden layers - random((2+1, 2+1)) : 3 x 3\n",
    "\n",
    "        for i in range(1, len(layers) - 1):\n",
    "            r = 2*np.random.random((layers[i-1] + 1, layers[i] + 1)) -1\n",
    "            self.weights.append(r)\n",
    "# output layer - random((2+1, 1)) : 3 x 1\n",
    "        r = 2*np.random.random( (layers[i] + 1, layers[i+1])) - 1\n",
    "        self.weights.append(r)\n",
    "\n",
    "    def fit(self, X, y, learning_rate=0.2, epochs=100000):\n",
    "# Add column of ones to X\n",
    "# This is to add the bias unit to the input layer\n",
    "        ones = np.atleast_2d(np.ones(X.shape[0]))\n",
    "        X = np.concatenate((ones.T, X), axis=1)\n",
    "        myList=[]\n",
    "        avList=[]\n",
    "\n",
    "\n",
    "        for k in range(epochs):\n",
    "            i = np.random.randint(X.shape[0])\n",
    "            a = [X[i]]\n",
    "    \n",
    "            for l in range(len(self.weights)):\n",
    "                dot_value = np.dot(a[l], self.weights[l])\n",
    "                activation = self.activation(dot_value)\n",
    "                a.append(activation)\n",
    "# output layer\n",
    "\n",
    "\n",
    "            error = y[i] - a[-1]\n",
    "            myList.append(np.sum(error**2))# mean squard error MSE\n",
    "            deltas = [error * self.activation_prime(a[-1])]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# we need to begin at the second to last layer\n",
    "# (a layer before the output layer)\n",
    "            for l in range(len(a) - 2, 0, -1):\n",
    "                deltas.append(deltas[-1].dot(self.weights[l].T)*self.activation_prime(a[l]))\n",
    "\n",
    "# reverse\n",
    "# [level3(output)->level2(hidden)] => [level2(hidden)->level3(output)]\n",
    "            deltas.reverse()\n",
    "\n",
    "# backpropagation\n",
    "# 1. Multiply its output delta and input activation\n",
    "# to get the gradient of the weight.\n",
    "# 2. Subtract a ratio (percentage) of the gradient from the weight.\n",
    "\n",
    "            cnt = 0\n",
    "            for i in range(len(self.weights)):\n",
    "                layer = np.atleast_2d(a[i])\n",
    "                delta = np.atleast_2d(deltas[i])\n",
    "                cnt =cnt + layer.T.dot(delta)\n",
    "                self.weights[i] =self.weights[i]+ learning_rate * cnt# save in smthing momentum\n",
    "            t = np.average(myList)\n",
    "            avList.append(t)\n",
    "            if k % 10000 == 0:\n",
    "                print('epochs:', k)\n",
    "                t = np.average(myList)\n",
    "                avList.append(t)\n",
    "\n",
    "# plt.show()\n",
    "#print(myList)\n",
    "\n",
    "#plt.plot(myList[1])\n",
    "        plt.plot(avList)\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('error')\n",
    "        plt.show()\n",
    "\n",
    "    def predict(self, x):\n",
    "\n",
    "        a = np.concatenate((np.ones(1).T, np.array(x)))\n",
    "\n",
    "        for l in range(0, len(self.weights)):\n",
    "            a = self.activation(np.dot(a, self.weights[l]))\n",
    "        return a\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    nn = NeuralNetwork([2,2,1])\n",
    "    X = np.array([[0, 0],\n",
    "                  [0, 1],\n",
    "                  [1, 0],\n",
    "                  [1, 1]])\n",
    "    y = np.array([0, 1, 1, 0])\n",
    "# X = np.array([[-1, -1],\n",
    "# [-1, 1],\n",
    "# [1, -1],\n",
    "# [1, 1]])\n",
    "# y = np.array([0, 1, 1, 0])\n",
    "\n",
    "    nn.fit(X, y)\n",
    "    for e in X:\n",
    "        print(e,nn.predict(e))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs: 0\n",
      "epochs: 10000\n",
      "epochs: 20000\n",
      "epochs: 30000\n",
      "epochs: 40000\n",
      "epochs: 50000\n",
      "epochs: 60000\n",
      "epochs: 70000\n",
      "epochs: 80000\n",
      "epochs: 90000\n",
      "[0 0] [ -7.14288447e-06]\n",
      "[0 1] [ 0.99526533]\n",
      "[1 0] [ 0.99524769]\n",
      "[1 1] [ -1.00120675e-05]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class NeuralNetwork:\n",
    "\n",
    "    def __init__(self, layers):\n",
    "        self.activation = tanh\n",
    "        self.activation_prime = tanh_prime\n",
    "\n",
    "        # Set weights\n",
    "        self.weights = []\n",
    "        # layers = [2,2,1]\n",
    "        # range of weight values (-1,1)\n",
    "        # input and hidden layers - random((2+1, 2+1)) : 3 x 3\n",
    "        \n",
    "        for i in range(1, len(layers) - 1):\n",
    "            r = 2*np.random.random((layers[i-1] + 1, layers[i] + 1)) -1\n",
    "            self.weights.append(r)\n",
    "        # output layer - random((2+1, 1)) : 3 x 1\n",
    "        r = 2*np.random.random( (layers[i] + 1, layers[i+1])) - 1\n",
    "        self.weights.append(r)\n",
    "\n",
    "    def fit(self, X, y, learning_rate=0.2, epochs=100000):\n",
    "        # Add column of ones to X\n",
    "        # This is to add the bias unit to the input layer\n",
    "        ones = np.atleast_2d(np.ones(X.shape[0]))\n",
    "        X = np.concatenate((ones.T, X), axis=1)\n",
    "         \n",
    "        for k in range(epochs):\n",
    "            i = np.random.randint(X.shape[0])\n",
    "            a = [X[i]]\n",
    "\n",
    "            for l in range(len(self.weights)):\n",
    "                    dot_value = np.dot(a[l], self.weights[l])\n",
    "                    activation = self.activation(dot_value)\n",
    "                    a.append(activation)\n",
    "            # output layer\n",
    "            error = y[i] - a[-1]\n",
    "            deltas = [error * self.activation_prime(a[-1])]\n",
    "\n",
    "            # we need to begin at the second to last layer \n",
    "            # (a layer before the output layer)\n",
    "            for l in range(len(a) - 2, 0, -1): \n",
    "                deltas.append(deltas[-1].dot(self.weights[l].T)*self.activation_prime(a[l]))\n",
    "\n",
    "            # reverse\n",
    "            # [level3(output)->level2(hidden)]  => [level2(hidden)->level3(output)]\n",
    "            deltas.reverse()\n",
    "\n",
    "            # backpropagation\n",
    "            # 1. Multiply its output delta and input activation \n",
    "            #    to get the gradient of the weight.\n",
    "            # 2. Subtract a ratio (percentage) of the gradient from the weight.\n",
    "            for i in range(len(self.weights)):\n",
    "                layer = np.atleast_2d(a[i])\n",
    "                delta = np.atleast_2d(deltas[i])\n",
    "                self.weights[i] += learning_rate * layer.T.dot(delta)\n",
    "\n",
    "            if k % 10000 == 0: \n",
    "                print('epochs:', k)\n",
    "\n",
    "    def predict(self, x): \n",
    "    \n",
    "        a = np.concatenate((np.ones(1).T, np.array(x)))      \n",
    "\n",
    "        for l in range(0, len(self.weights)):\n",
    "            a = self.activation(np.dot(a, self.weights[l]))\n",
    "        return a\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    nn = NeuralNetwork([2,2,1])\n",
    "    X = np.array([[0, 0],\n",
    "                  [0, 1],\n",
    "                  [1, 0],\n",
    "                  [1, 1]])\n",
    "    y = np.array([0, 1, 1, 0])\n",
    "#     X = np.array([[-1, -1],\n",
    "#                   [-1, 1],\n",
    "#                   [1, -1],\n",
    "#                   [1, 1]])\n",
    "#     y = np.array([0, 1, 1, 0])\n",
    "\n",
    "    nn.fit(X, y)\n",
    "    for e in X:\n",
    "        print(e,nn.predict(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
